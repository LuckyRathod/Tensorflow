{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will explore how to create an example environment with OpenAI Gym.\n",
    "Goal of Cartpole Environment is to balance the pole on cart.\n",
    "Actions allow us to move the cart left and right to attempt to balance pole.\n",
    "Environment is numpy array with 4 floating point numbers .\n",
    "-Horizontal Position\n",
    "-Horizontal Velocity\n",
    "-Angle of pole\n",
    "-Angular Velocity.\n",
    "\n",
    "You can grab these values from environment and use them for your agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucky_Rathod\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "print(gym.__version__)\n",
    "\n",
    "# Make the environment, replace this string with any\n",
    "# from the docs. (Some environments have dependencies)\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Reset the environment to default beginning\n",
    "env.reset()\n",
    "\n",
    "# Using _ as temp placeholder variable\n",
    "# Render Environment for 1000 timesteps\n",
    "# We are not using for var therfore _ is used\n",
    "for _ in range(1000):\n",
    "    # Render the env\n",
    "    \n",
    "    #Render will create pop up that allows you to view environment\n",
    "    env.render()\n",
    "\n",
    "    # Still a lot more explanation to come for this line!\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "    \n",
    "'''\n",
    "Above Code will create a window in which cart will move from one location to another\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPEN AI GYM Observations\n",
    "\n",
    "'''\n",
    "step funtions returns usefull objects for our agent .\n",
    "\n",
    "1. Observation\n",
    "\n",
    "Env specific information representing environment observation.\n",
    "Examples - Angles,Velocities,Game States\n",
    "\n",
    "2. Reward\n",
    "\n",
    "Amount of reward achieved by previous action.Agent will have to increase reward.\n",
    "\n",
    "3. Done\n",
    "\n",
    "Boolean variable indicating whether environment needs to be reset\n",
    "Example - Game is Lost,Pole tipped over\n",
    "\n",
    "4. Info \n",
    "\n",
    "Dictionary object with debugging information\n",
    "\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation\n",
      "Performed One Random Action\n",
      "\n",
      "\n",
      "observation\n",
      "[-0.03960773  0.18285529  0.00824508 -0.27788961]\n",
      "\n",
      "\n",
      "reward\n",
      "1.0\n",
      "\n",
      "\n",
      "done\n",
      "False\n",
      "\n",
      "\n",
      "info\n",
      "{}\n",
      "\n",
      "\n",
      "Performed One Random Action\n",
      "\n",
      "\n",
      "observation\n",
      "[-0.03595062 -0.01238331  0.00268728  0.01738236]\n",
      "\n",
      "\n",
      "reward\n",
      "1.0\n",
      "\n",
      "\n",
      "done\n",
      "False\n",
      "\n",
      "\n",
      "info\n",
      "{}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Make the environment, replace this string with any\n",
    "# from the docs. (Some environments have dependencies)\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "print(\"Initial Observation\")\n",
    "# Reset the environment to default beginning\n",
    "observation = env.reset()\n",
    "\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    #Will not render in these code\n",
    "    action = env.action_space.sample()\n",
    "    observation,reward,done,info = env.step(action) # take a random action\n",
    "    \n",
    "    print(\"Performed One Random Action\")\n",
    "    print('\\n')\n",
    "    print('observation')\n",
    "    print(observation)\n",
    "    print('\\n')\n",
    "\n",
    "    print('reward')\n",
    "    print(reward)\n",
    "    print('\\n')\n",
    "\n",
    "    print('done')\n",
    "    print(done)\n",
    "    print('\\n')\n",
    "\n",
    "    print('info')\n",
    "    print(info)\n",
    "    print('\\n')\n",
    "\n",
    "    \n",
    "#These Gives an output of 4 Elements in array\n",
    "#It represents that poll standing straight up with the cart in middle position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open AI Gym Actions\n",
    "\n",
    "Now we will create a simple policy .Move the cart to right if pole falls to right \n",
    "and vice versa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "## There are only Two Actions as you can see from print\n",
    "print(env.action_space)\n",
    "# #> Discrete(2)\n",
    "\n",
    "## There are 4 observations \n",
    "print(env.observation_space)\n",
    "# #> Box(4,)\n",
    "\n",
    "#### Initial Observation    \n",
    "observation = env.reset()\n",
    "\n",
    "for t in range(1000):\n",
    "\n",
    "    #env.render()\n",
    "\n",
    "    # 4 Observation\n",
    "    cart_pos , cart_vel , pole_ang , ang_vel = observation\n",
    "\n",
    "    # Move Cart Right if Pole is Falling to the Right\n",
    "\n",
    "    # Angle is measured off straight vertical line\n",
    "    if pole_ang > 0:\n",
    "        # Move Right\n",
    "        action = 1\n",
    "    else:\n",
    "        # Move Left\n",
    "        action = 0\n",
    "\n",
    "    # Perform Action\n",
    "    observation , reward, done, info = env.step(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Simple Neural Network that takes in observation array passes it through hidden layers\n",
    "and outputs 2 probabilities,One for Left and another for right\n",
    "\n",
    "Notice we dont just automatically choose the highest probability for our decision\n",
    "This is to balance trying out new actions versus constantly choosing well known actions\n",
    "\n",
    "\n",
    "Once we understand this network , we will explore how to take into account historic \n",
    "actions by learning about Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-1-f96a33026afd>:24: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Lucky_Rathod\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-1-f96a33026afd>:34: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "Done after 19 steps\n",
      "Done after 14 steps\n",
      "Done after 15 steps\n",
      "Done after 23 steps\n",
      "Done after 16 steps\n",
      "Done after 11 steps\n",
      "Done after 11 steps\n",
      "Done after 16 steps\n",
      "Done after 17 steps\n",
      "Done after 16 steps\n",
      "Done after 33 steps\n",
      "Done after 10 steps\n",
      "Done after 16 steps\n",
      "Done after 12 steps\n",
      "Done after 11 steps\n",
      "Done after 14 steps\n",
      "Done after 14 steps\n",
      "Done after 9 steps\n",
      "Done after 42 steps\n",
      "Done after 11 steps\n",
      "Done after 31 steps\n",
      "Done after 10 steps\n",
      "Done after 15 steps\n",
      "Done after 17 steps\n",
      "Done after 21 steps\n",
      "Done after 22 steps\n",
      "Done after 9 steps\n",
      "Done after 75 steps\n",
      "Done after 13 steps\n",
      "Done after 34 steps\n",
      "Done after 11 steps\n",
      "Done after 14 steps\n",
      "Done after 17 steps\n",
      "Done after 11 steps\n",
      "Done after 38 steps\n",
      "Done after 15 steps\n",
      "Done after 39 steps\n",
      "Done after 11 steps\n",
      "Done after 14 steps\n",
      "Done after 9 steps\n",
      "Done after 37 steps\n",
      "Done after 17 steps\n",
      "Done after 17 steps\n",
      "Done after 47 steps\n",
      "Done after 14 steps\n",
      "Done after 43 steps\n",
      "Done after 14 steps\n",
      "Done after 10 steps\n",
      "Done after 10 steps\n",
      "Done after 34 steps\n",
      "After 50 episodes the average cart steps before done was 19.98\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "###############################################\n",
    "######## PART ONE: NETWORK VARIABLES #########\n",
    "#############################################\n",
    " \n",
    "# Observation Space has 4 inputs\n",
    "num_inputs = 4\n",
    "\n",
    "num_hidden = 4\n",
    "\n",
    "# Outputs the probability it should go left\n",
    "num_outputs = 1\n",
    "\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "\n",
    "###############################################\n",
    "######## PART TWO: NETWORK LAYERS #########\n",
    "#############################################\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None,num_inputs])\n",
    "hidden_layer_one = tf.layers.dense(X,num_hidden,activation=tf.nn.relu,kernel_initializer=initializer)\n",
    "hidden_layer_two = tf.layers.dense(hidden_layer_one,num_hidden,activation=tf.nn.relu,kernel_initializer=initializer)\n",
    "\n",
    "# Probability to go left\n",
    "output_layer = tf.layers.dense(hidden_layer_two,num_outputs,activation=tf.nn.sigmoid,kernel_initializer=initializer)\n",
    "\n",
    "# [ Prob to go left , Prob to go right]\n",
    "probabilties = tf.concat(axis=1, values=[output_layer, 1 - output_layer])\n",
    "\n",
    "# Sample 1 randomly based on probabilities\n",
    "action = tf.multinomial(probabilties, num_samples=1)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "###############################################\n",
    "######## PART THREE: SESSION #########\n",
    "#############################################\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "epi = 50\n",
    "step_limit = 500\n",
    "avg_steps = []\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i_episode in range(epi):\n",
    "        obs = env.reset()\n",
    "\n",
    "        for step in range(step_limit):\n",
    "            # env.render()\n",
    "            action_val = action.eval(feed_dict={X: obs.reshape(1, num_inputs)})\n",
    "            \n",
    "            #action_val[0][0] - # 0 or 1\n",
    "            obs, reward, done, info = env.step(action_val[0][0])\n",
    "            if done:\n",
    "                avg_steps.append(step)\n",
    "                print('Done after {} steps'.format(step))\n",
    "                break\n",
    "print(\"After {} episodes the average cart steps before done was {}\".format(epi,np.mean(avg_steps)))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUR previous network didnt performed well . This maybe because we arent considering the history\n",
    "of our actions , we are only consideringg a single previous action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "### VARIABLES ###########\n",
    "########################\n",
    "\n",
    "num_inputs = 4\n",
    "num_hidden = 4\n",
    "num_outputs = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "\n",
    "#################################\n",
    "### CREATING THE NETWORK #######\n",
    "###############################  \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_inputs])\n",
    "\n",
    "hidden_layer = tf.layers.dense(X, num_hidden, activation=tf.nn.elu, kernel_initializer=initializer)\n",
    "logits = tf.layers.dense(hidden_layer, num_outputs) \n",
    "outputs = tf.nn.sigmoid(logits)  # probability of action 0 (left)\n",
    "\n",
    "probabilties = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
    "action = tf.multinomial( probabilties, num_samples=1)\n",
    "\n",
    "# Convert from Tensor to number for network training\n",
    "y = 1. - tf.to_float(action)\n",
    "\n",
    "########################################\n",
    "### LOSS FUNCTION AND OPTIMIZATION ####\n",
    "######################################\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# https://stackoverflow.com/questions/41954198/optimizer-compute-gradients-how-the-gradients-are-calculated-programatically\n",
    "# https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "\n",
    "\n",
    "################################\n",
    "#### GRADIENTS ################\n",
    "##############################\n",
    "gradients_and_variables = optimizer.compute_gradients(cross_entropy)\n",
    "\n",
    "\n",
    "\n",
    "gradients = []\n",
    "gradient_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "\n",
    "for gradient, variable in gradients_and_variables:\n",
    "    gradients.append(gradient)\n",
    "    gradient_placeholder = tf.placeholder(tf.float32, shape=gradient.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "\n",
    "\n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "######################################\n",
    "#### REWARD FUNCTIONs ################\n",
    "####################################\n",
    "# CHECK OUT: https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724\n",
    "\n",
    "def helper_discount_rewards(rewards, discount_rate):\n",
    "    '''\n",
    "    Takes in rewards and applies discount rate\n",
    "    '''\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    '''\n",
    "    Takes in all rewards, applies helper_discount function and then normalizes\n",
    "    using mean and std.\n",
    "    '''\n",
    "    all_discounted_rewards = []\n",
    "    for rewards in all_rewards:\n",
    "        all_discounted_rewards.append(helper_discount_rewards(rewards,discount_rate))\n",
    "\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]\n",
    "\n",
    "########################################\n",
    "#### TRAINING SESSION #################\n",
    "######################################\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "num_game_rounds = 10\n",
    "max_game_steps = 1000\n",
    "num_iterations = 200\n",
    "discount_rate = 0.95\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(\"Currently on Iteration: {} \\n\".format(iteration) )\n",
    "\n",
    "        all_rewards = []\n",
    "        all_gradients = []\n",
    "\n",
    "        # Play n amount of game rounds\n",
    "        for game in range(num_game_rounds):\n",
    "\n",
    "            current_rewards = []\n",
    "            current_gradients = []\n",
    "\n",
    "            observations = env.reset()\n",
    "\n",
    "            # Only allow n amount of steps in game\n",
    "            for step in range(max_game_steps):\n",
    "\n",
    "                # Get Actions and Gradients\n",
    "                action_val, gradients_val = sess.run([action, gradients], feed_dict={X: observations.reshape(1, num_inputs)})\n",
    "\n",
    "                # Perform Action\n",
    "                observations, reward, done, info = env.step(action_val[0][0])\n",
    "\n",
    "                # Get Current Rewards and Gradients\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "\n",
    "                if done:\n",
    "                    # Game Ended\n",
    "                    break\n",
    "\n",
    "            # Append to list of all rewards\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "\n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards,discount_rate)\n",
    "        feed_dict = {}\n",
    "\n",
    "\n",
    "        for var_index, gradient_placeholder in enumerate(gradient_placeholders):\n",
    "            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n",
    "                                      for game_index, rewards in enumerate(all_rewards)\n",
    "                                          for step, reward in enumerate(rewards)], axis=0)\n",
    "            feed_dict[gradient_placeholder] = mean_gradients\n",
    "\n",
    "        sess.run(training_op, feed_dict=feed_dict)\n",
    "\n",
    "    print('SAVING GRAPH AND SESSION')\n",
    "    meta_graph_def = tf.train.export_meta_graph(filename='/new_saved_models/my-650-step-model.meta')\n",
    "    saver.save(sess, '/new_saved_models/my-650-step-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################\n",
    "### RUN TRAINED MODEL ON ENVIRONMENT #######\n",
    "###########################################\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "observations = env.reset()\n",
    "with tf.Session() as sess:\n",
    "    # https://www.tensorflow.org/api_guides/python/meta_graph\n",
    "    new_saver = tf.train.import_meta_graph(r'C:\\Users\\Lucky_Rathod\\Desktop\\Udemy Courses\\Complete Tensorflow\\Original Notes\\Tensorflow-Bootcamp-master\\07-Reinforcement-Learning-OpenAI\\models\\cartpole_model.ckpt.meta')\n",
    "    new_saver.restore(sess,r'C:\\Users\\Lucky_Rathod\\Desktop\\Udemy Courses\\Complete Tensorflow\\Original Notes\\Tensorflow-Bootcamp-master\\07-Reinforcement-Learning-OpenAI\\models\\cartpole_model.ckpt')\n",
    "\n",
    "    for x in range(500):\n",
    "        env.render()\n",
    "        action_val, gradients_val = sess.run([action, gradients], feed_dict={X: observations.reshape(1, num_inputs)})\n",
    "        observations, reward, done, info = env.step(action_val[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
